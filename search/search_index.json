{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Spark studies","text":"<p>Update</p> <p>Sept-05-2023</p> <p>Spark started at UC Berkeley in 2009, and it is one of the most adopted open source solution to run parallel data processing on distributed systems to process large-scale data.</p> <p>The goal of Spark is to offer an unified platform for writing big data application: this means consistent APIs to do data loading, SQL, streaming, machine learning... </p> <p>From the business point of view, users often seek to extract meaningful insights from their data and convert it into a format that can be utilized by business intelligence applications or for training machine models. Sparks, in this context, serves as a valuable tool for constructing data pipelines that facilitate the transformation of raw data. These pipelines enable the filtering, cleansing, and augmentation of data sets, empowering them with analytics and business-level aggregates. Ultimately, this results in data sets that are readily consumable by various business applications.</p> <p></p> <p>The unified model for file system access is Hadoop FS.</p> <p>The commercial version is supported by Databricks, but it is also available as managed services by cloud providers, such as AWS (via EMR).</p> <p>Data Scientists use Python Pandas and scikit-learn to do most of their work on one computer, when the memory of one computer is not enough then Spark helps to process such big data. Pyspark is the Python API for Spark.</p>"},{"location":"#characteristics","title":"Characteristics","text":"<p>Apache Spark boasts several key characteristics that make it a powerful tool for data analysis and big data workload processing.</p> <ol> <li> <p>Data locality and computation: Spark recognizes the costliness of moving data and focuses on performing computations directly on the data, regardless of its location. This approach minimizes unnecessary data transfers and enhances processing efficiency.</p> </li> <li> <p>Unified API for data analysis: To simplify the development process, Spark provides a unified API that supports common data analysis tasks. Two primary abstractions, RDD (Resilient Distributed Dataset) and DataFrame, facilitate seamless data manipulation and transformation. You can explore more about these abstractions in this section.</p> </li> <li> <p>Cluster-based architecture: the architecture includes a single <code>manager</code> node and multiple <code>executor</code> nodes. By adding more executor nodes, Spark can horizontally scale, enabling efficient processing of large-scale datasets.</p> </li> <li> <p>RDD and DataFrames: RDD, the main data element in Spark, acts as an abstraction for managing distributed data within the Spark cluster. Each dataset within an RDD is logically partitioned, allowing computations to be distributed across different cluster nodes. In the newest version of Spark, DataFrames provide a more structured and optimized approach for data manipulation.</p> </li> <li> <p>Comprehensive libraries: Spark comes equipped with a range of libraries tailored to specific use cases. Spark SQL enables seamless querying and manipulation of structured data. MLlib empowers large-scale machine learning with support for iterative algorithms. Spark Streaming and Structured Streaming facilitate real-time and stream processing. Lastly, GraphX enables efficient graph analytics.</p> <p></p> </li> <li> <p>Scalable machine learning: Spark's architecture and distributed computing capabilities make it a powerful platform for large-scale machine learning. It efficiently handles iterative algorithms that require multiple passes over the data, enabling the processing of vast datasets with relative ease.</p> </li> <li> <p>Directed Acyclic Graph (DAG) execution model: Spark utilizes a DAG to define the workflow of computations performed on the executor nodes. This workflow is optimized by a DAG engine, which ensures efficient execution. Developers write code that gets mapped to the DAG for seamless processing.</p> </li> <li> <p>Scala as the recommended language: While Spark supports multiple programming languages, it is primarily written in Scala. For the best performance and development experience, it is recommended to develop Spark applications using Scala. However, Python is a viable solution for proof-of-concepts (POCs) and prototyping.</p> </li> <li>Exceptional performance: Spark's in-memory processing and optimized execution engine contribute to its impressive speed. In fact, Spark is known to be up to 100 times faster than Hadoop MapReduce, making it an ideal choice for demanding big data workloads.</li> </ol>"},{"location":"#architecture","title":"Architecture","text":"<ul> <li> <p>Spark Applications consist of a driver process and a set of executor processes. The driver process runs the <code>main()</code> function, sits on a node in the cluster, and is responsible for three things:</p> <ul> <li>maintain information about the Spark Application.</li> <li>respond to a user\u2019s program or input.</li> <li>analyze, distribute, and schedule work across the executors.</li> </ul> </li> <li> <p>Each executor is responsible for only two things: executing code assigned to it by the driver, and reporting the state of the computation on that executor back to the driver node.</p> </li> </ul> <p></p> <ul> <li>The main entry point for programming is the <code>SparkSession</code> object:</li> </ul> <pre><code>spark = SparkSession.builder.appName(\"PopularMovies\").getOrCreate()\n\nlines = spark.sparkContext.textFile(\"../data/movielens/u.data\")\n</code></pre> <p>See more development practices in this other article.</p>"},{"location":"#extended-architecture","title":"Extended architecture","text":"<p>The full architecture includes distributed storage using data lake technology and the Delta Lake protocol which is an intermediate layer between Spark and storage to add services like ACID transaction and metadata about the object in the data lake. Spark job uses the Delta Lake to read and write data. </p> <p></p> Delta Lake <p>Delta Lake is an open-source storage layer that is designed to bring reliability and scalability to big data processing. It provides ACID transactional capabilities to data lakes.</p> <ol> <li> <p>Reliability: Delta Lake ensures data integrity and reliability by providing ACID transactions, which means that multiple concurrent readers and writers can access the data without conflicts or inconsistencies. It also offers schema enforcement to prevent data corruption.</p> </li> <li> <p>Scalability: Delta Lake leverages the distributed processing power of Apache Spark to handle large-scale data processing. It supports parallel processing, enabling efficient execution of queries on massive datasets.</p> </li> <li> <p>Data versioning: Delta Lake maintains a full history of all changes made to the data, allowing you to track and query data at any point in time. This feature is particularly useful for auditing and compliance purposes.</p> </li> <li> <p>Time travel: Delta Lake enables you to query data as it existed at a specific point in time, making it easy to revert to previous versions or analyze data changes over time. It provides a simple syntax to perform time travel queries without the need for complex data restoration processes.</p> </li> <li> <p>Schema evolution: Delta Lake allows you to evolve the schema of your data over time, providing flexibility when dealing with changing data structures. It supports schema validation and evolution, ensuring data consistency and compatibility.</p> </li> <li> <p>Optimized performance: Delta Lake optimizes data storage and processing by using a combination of file-level and data-level optimizations. It employs techniques like file compaction, data skipping, and predicate pushdown to improve query performance and reduce storage costs.</p> </li> </ol>"},{"location":"#considerations","title":"Considerations","text":"<ul> <li>Spark executors are not really cattle as they are keeping data partitions. So from a pure Spark architecture, a kubernetes deployment, may look like an anti-pattern. RDD should help to compensate for pod failure.</li> </ul> <p>Next step... Getting started &gt;&gt;&gt;</p>"},{"location":"cloud-storage/","title":"IBM Cloud Object Storage","text":"<p>IBM Cloud Object Storage is a cloud based storage for backup, application data files, persistence storage for analytics.</p>"},{"location":"cloud-storage/#characteristics","title":"Characteristics","text":"<ul> <li>Cloud Object Storage is a multi-tenant system, and all instances of Object Storage share physical infrastructure</li> <li>Available with three types of resiliency: Cross Region (EU, AP, US), Regional (multiple availability one in a region), and Single Data Center (multiple servers)</li> <li>COS offers REST-based API for reading and writing objects: each storage location has its own set of URLs.</li> <li><code>Bucket</code> is the container of the data and defines metadata about the storage: resiliency type, scale factor..</li> </ul> <ul> <li>All buckets in all regions across the globe share a single namespace.</li> <li>Bucket configuration defines role based (writer, reader, manager) to control access per user</li> <li>To get access to the content of a bucket using a application, define a <code>ServiceID</code>, under the bucket configuration and the <code>Access policies</code> to control the access rules, or link an existing service ID to the bucket.</li> </ul>"},{"location":"cloud-storage/#access-via-cli","title":"Access via CLI","text":"<ul> <li>Login to IBM Cloud: <code>ibmcloud login -u .... --sso</code></li> <li>Install cloud object storage plugin if not done before, or if you need to update existing version: <code>ibmcloud plugin install cloud-object-storage</code></li> <li>To use the COS plugin, we need to configure it with the region to use and the objectstore\u2018s unique number, known as a CRN. To get the CRN do the following command using the name of the COS service: <code>ibmcloud resource service-instance \"Cloud Object Storage-yj\"</code></li> </ul> <pre><code>  Retrieving service instance Cloud Object Storage-yj in all resource groups under account Cloud Client Engagement Team's Account as boyerje@us.ibm.com...\n    OK\n\n    Name:                  Cloud Object Storage-yj   \n    ID:                    crn:v1:bluemix:public:cloud-object-storage:global:a/98010327e7....   \n</code></pre> <ul> <li><code>ibmcloud cos config crn</code> then enter the crn from previous command in the prompt.</li> <li>Get the list of buckets: <code>ibmcloud cos list-buckets</code></li> <li>Creating a bucket (name needs to be globally unique): <code>ibmcloud cos create-bucket --bucket mybucket</code></li> </ul>"},{"location":"cloud-storage/#access-via-api","title":"Access via API","text":"<ul> <li>Get the API bearer token</li> </ul> <pre><code> export COS_ADMIN_TOKEN=`ibmcloud iam oauth-tokens | grep IAM | awk '{printf(\"%s\", $4)}'`\n</code></pre> <ul> <li> <p>Get service instance ID (replace \"Cloud Object Storage-yj\" with your service instance name): <code>export COS_GUID=$(ibmcloud resource service-instance \"Cloud Object Storage-yj\" | grep GUID | awk '{printf(\"%s\", $2)}')</code></p> </li> <li> <p>Using curl to access the content of a bucket:</p> </li> </ul> <pre><code>    curl https://s3.us-south.cloud-object-storage.appdomain.cloud/cloud-object-storage-eda-folders -H \"Authorization: Bearer $COS_ADMIN_TOKEN\"  -H \"x-amz-acl: public-read\" -H \"ibm-service-instance-id: $COS_GUID\"\n</code></pre> <p></p> <ul> <li>Download a file (like access_log.txt): <code>curl -X GET https://s3.us-south.cloud-object-storage.appdomain.cloud/cloud-object-storage-eda-folders/access_log.txt -H \"Authorization: Bearer $COS_ADMIN_TOKEN\"  -H \"x-amz-acl: public-read\" -H \"ibm-service-instance-id: $COS_GUID\"</code></li> <li>Other curl commands explanation.</li> </ul>"},{"location":"cloud-storage/#fearther-readings","title":"Fearther readings","text":"<ul> <li>Getting started</li> <li>Analyze data faster using Spark and Cloud Object Storage tutorial</li> </ul>"},{"location":"compendium/","title":"Apache Sparks interresting articles to read","text":"<ul> <li>Main apache site</li> <li>Udemy training on spark for beginner from Frank Kane</li> <li>Spark by examples a comprehensive list of Spark features</li> <li>Playground EMR and Spark</li> <li>Udemy training on spark streaming</li> <li>Spark Streaming programming guide</li> <li>Analyze data faster using Spark and Cloud Object Storage tutorial</li> </ul>"},{"location":"scala_summary/","title":"Scala summary","text":"<p>Scala is a blend of object-oriented and functional programming concepts in a statically typed language. Everything is an object, including numbers or functions. Function types are classes that can be inherited by subclasses. It runs on the JVM.</p> <p>OOP is orthogonal from logic programming, functional or imperative programming.</p> <ul> <li>Run on the standard Java platform and interoperates seamlessly with all Java libraries</li> <li>It is also a scripting language with its own interpreter</li> <li>Support an \"associative map\" construct in the syntax of the language</li> <li>Scala allows users to grow and adapt the language in the directions they need by defining easy-to-use libraries that feel like native language support.</li> <li>Support enhanced parallel processing using actors: Actors are concurrency abstractions that can be implemented on top of threads. They communicate by sending messages to each other. An actor can perform two basic operations, message send and receive. The send operation, denoted by an exclamation point (!), sends a message to an actor. A send is asynchronous.</li> <li>Scala is an object-oriented language in pure form: every value is an object and every operation is a method call.</li> <li>It supports composing objects with the concept of Trait. Traits are like interfaces in Java, but they can also have method implementations and even fields. Objects are constructed by mixing composition, which takes the members of a class and adds the members of a number of traits to them.</li> <li>Traits are more \"pluggable\" than classes</li> <li>Functions that are first-class values provide a convenient means for abstracting over operations and creating new control structures</li> <li>The second main idea of functional programming is that the operations of a program should map input values to output values rather than change data in place</li> <li>Methods should not have any side effects. They should communicate with their environment only by taking arguments and returning results</li> </ul>"},{"location":"scala_summary/#why-scala","title":"why scala ?","text":"<ul> <li>Scala is compatible with java API</li> <li>Concise: quicker to write, easier to read, and most importantly, less error prone</li> <li>High level: scala lets developers raise the level of abstraction in the interfaces they design and use</li> <li>Scala's functional programming style also offers high-level reasoning principles for programming.</li> <li>Statically typed: very advanced static type system. Starting from a system of nested class types much like Java's, it allows you to parameterize types with generics, to combine types using intersections, and to hide details of types using abstract types. Verbosity is avoided through type inference and flexibility is gained through pattern matching and several new ways to write and compose types</li> <li>Static type systems can prove the absence of certain run-time errors</li> <li>A static type system provides a safety net that lets you make changes to a codebase with a high degree of confidence, by identifying change name, adding new parameters to functions...</li> </ul> <p>Scala has some innovations: abstract types provide a more object-oriented alternative to generic types, its traits allow for flexible component assembly, and its extractors provide a representation-independent way to do pattern matching.</p> <p>Scala has an interactive shell or REPL: Read-Eval-Print-Loop. Started by using &gt; scala or using the scala build tool, sbt, using &gt;sbt console.</p> <p>To read more about scala API: www.scala-lang.org/api</p>"},{"location":"scala_summary/#language-constructs","title":"language constructs","text":""},{"location":"scala_summary/#variables","title":"Variables","text":"<p>Scala has two kinds of variables, vals and vars. A val is similar to a final variable in Java. Once initialized, a val can never be reassigned. A var, by contrast, is similar to a non-final variable in Java. Scala uses the type inference capability to figure out the types not specified at variable declaration:</p> <pre><code>val msg = \"Hi\"\nvar x = 4\n</code></pre> <p>See some first exercise in this code: LeaningScala1.sc</p> <p>Primitive types are with uppercase because Scala is fully object oriented</p> <p>Object is for a class that is a single instance. there is no static concept in Scala. it is covered by the object concept</p>"},{"location":"scala_summary/#expression-evaluation","title":"Expression evaluation","text":"<p>non-primitive expression is evaluated:</p> <ul> <li>take leftmost operator</li> <li>evaluate its operands left before right</li> <li>apply the operator to the operands</li> </ul>"},{"location":"scala_summary/#operators-are-methods","title":"Operators are methods","text":"<p>1 + 2 really means the same thing as (1).+(2). The + symbol is an operator\u2014an infix operator to be specific. Operator notation is not limited to methods like + that look like operators in other languages. You can use any method in operator notation.</p> <p>Scala also has two other operator notations: prefix and postfix. In prefix notation, you put the method name before the object on which you are invoking the method. prefix and postfix operators are unary: they take just one operand. The only identifiers that can be used as prefix operators are +, -, !, and ~</p> <p>The logical-and and logical-or operations are short-circuited as in Java: expressions built from these operators are only evaluated as far as needed to determine the result</p> <p>The operator precedence looks like:</p> <pre><code>(all other special characters)\n* / %\n+ -\n:\n=!\n&lt;&gt;\n&amp;\n^\n|\n(all letters)\n(all assignment operators)\n</code></pre>"},{"location":"scala_summary/#function","title":"Function","text":"<p>Function declaration has the following form</p> <pre><code>def functionName ([list of parameters]) : [return type]\ndef constOne(x: Int, y: =&gt; Int = 1\n// y is passed by name\n\ndef format now\n</code></pre> <p>A function can support using Type parameter, written in []  See polymorphism</p> <pre><code>def singleton[T](elemen: T) : T\n\nor\n def encode[T](l : List[T]): List[(T,Int)]\n</code></pre> <p>Function evaluation for a function f(a1,...,an) is done by evaluating the expressions a1,...,an of the parameters resulting in the values v1,..,vn, then by replacing the application with the body of the function f in which the actual parameters are v1,..,vn</p> <p>function evaluation is done by substitution model: all evaluation done is by reducing an expression to a value.</p> <ul> <li>evaluate all function arguments from left to right</li> <li>replace function application by function right-hand side</li> <li>replace the formal parameters of the function by the actual arguments</li> <li>two evaluation models: call by value, and call by name</li> </ul> <p>See the code example in: LeaningScala3.sc</p> <p>A name is evaluate by replacing its right end definition.</p> <p>Functions are objects. It is possible to pass functions as arguments, to store them in variables, and to return them from other functions.</p> <pre><code> // function that uses a function as argument\n    // type () =&gt; Unit is for function taking no argument and return nothing\n     def oncePerSecond(callback: () =&gt; Unit) {\n          // call it\n          callback()\n     }\n// factorial example\ndef factorial(n : Int): Int =\n   if (n == 0) 1 else n * factorial (n-1)\n\n// greatest common denominator using the Euclid algorithm\ndef gcd(a: Int, b: Int) : Int = \n   if (b == 0) a else gcd(b,a % b)\n\n// sum of integers between a and b with function as argument\n  // f is a function as A=&gt;B is a type of a function that takes an argument\n  // of type A and returns a result of type B\n  def sum(f : Int =&gt; Int, a: Int, b: Int) : Int =\n    if (a &gt; b) 0\n    else f(a) + sum(f,a+ 1,b)\n\n def sumInts(a : Int,b: Int) = sum(id,a,b)\n def id(x: Int): Int = x\n</code></pre> <p>The sum function is a linear recursion. Tail recursion: if a function calls itself as its last action, the functions stack frame can be used. It is iterative. (gcd above is a tail recursive function)</p> <p>Anonymous function:</p> <pre><code>// Declared like:  (x1 : T1,.... , xn :Tn) =&gt; E\n\n(x:Int) =&gt; x*x*x\n// using anonymous function to compute the sum of the cubes using the sum function defined above\ndef sumCubes(a : Int,b: Int) = sum(x =&gt; x *x *x,a,b)\n\n// another simplification by using function as return result\n// sum return a function that takes to int as argument and return a int\ndef sum(f: Int =&gt; Int): (Int, Int) =&gt; Int = {\n         def sumF(a:Int, b:Int) : Int =\n           if (a &gt; b)  0\n           else f(a) + sumF(a+1 ,b)\n         sumF\n    }\n\ndef sumCubes = sum(x =&gt; x*x*x)\nprintln(sumCubes(1,10))\n</code></pre> <p>When doing functional programming, you try to avoid side effect, by using vars. When your function need to modify its parameters, use the Unit result type. Function values are treated as objects in Scala.</p>"},{"location":"scala_summary/#class","title":"Class","text":"<p>The class declaration includes the parameters of the constructor:</p> <pre><code>class Tweet(val user: String, val text: String, val retweets: Int) { ...\n</code></pre> <p>parameters are always pass by-value, they become fields of the class</p> <pre><code>val a = new Tweet(\"gizmodo\",\" some text\", 20)\na.user\n</code></pre> <p>Classes in Scala cannot have static members. So there is the concept of singleton object: the definition looks like a class definition, except instead of the keyword <code>class</code> you use the keyword <code>object</code>. When a singleton object shares the same name with a class, it is called that class's companion object. They cannot take parameters. A singleton object that does not share the same name with a companion class is called a standalone object.</p> <p>It is possible to define precondition of the primary constructor of a class. For example for the Rational class we can enforce that the denominator is never 0 using require expression</p> <pre><code>class Rational(n: Int, d: Int) {\n    require(d != 0)\n    override def toString = n +\"/\"+ d\n  }\n</code></pre> <p>Class can use type parameter to support polymorphism</p> <pre><code>trait List[T] {\n  def isEmpty : Boolean\n  def head : T\n  def tail : List[T]\n}\n\nclass Cons[T]( val head : T, tail: List[T]) extends List[T] {\n  def isEmpty = false\n}\n\nclass Nil[T] extends List[T] {\n def isEmpty = true\n def head :Nothing =  throw new NoSuchElementException(\"Nil head\")\n def tail :Nothing = throw new NoSuchElementException(\"Nil tail\")\n}\n</code></pre> <p>Immutable objects offer several advantages over mutable objects:</p> <ul> <li>easier to reason about than mutable ones, because they do not have complex state spaces that change over time</li> <li>pass immutable objects around quite freely, whereas you may need to make defensive copies of mutable objects before passing them to other code</li> <li>there is no way for two threads concurrently accessing an immutable to corrupt its state once it has been properly constructed, because no thread can change the state of an immutable!</li> <li>immutable objects make safe hashtable keys</li> <li>main disadvantage of immutable objects is that they sometimes require that a large object graph be copied where otherwise an update could be done in place.</li> </ul> <p><code>scala.Any</code> is the root of all classes in scala. <code>scala.AnyVal</code> is base type of all primitive types. <code>scala.AnyRef</code> the base type of reference types, it is an alias to java.lang.Object.</p> <p><code>scala.Nothing</code> is at bottom of type hierarchy. There is no value of type Nothing. it is used to signal abnormal termination or as an element type of an empty collection. <code>Set[Nothing]</code></p> <p>As the function throw and exception, the return type of the function is set to Nothing.</p> <pre><code>scala&gt; def error(msg : String) = throw new Error(msg)\nerror: (msg: String)Nothing\n</code></pre> <p><code>scala.Null</code>, is subclass of all the classes in AnyRef, and it is the type of null value. Null is incompatible with subtypes of <code>AnyVal</code>.</p> <pre><code>scala&gt; val x = null\nx: Null = null\n</code></pre> <p>One important characteristic of method parameters in Scala is that they are vals, not vars</p>"},{"location":"scala_summary/#unit","title":"Unit","text":"<p>Methods with a result type of Unit, are executed for their side effect.  A side effect is generally defined as mutating state somewhere external to the method or performing an I/O action.</p>"},{"location":"scala_summary/#trait","title":"Trait","text":"<p>Declared like an abstract class. They contains fields and concrete methods. A class can inherit one unique class but can have many traits. Trait do not have parameters as part of the constructor.</p>"},{"location":"scala_summary/#pattern-matching","title":"Pattern matching","text":"<p>When using a pure OO implementation, adding a new method to a class you may add this method in multiple classes and subclasses.  Scala offers the pattern matching capability to support simple function extension.</p> <p><code>case</code> classes are like <code>class</code> but the case keyword let them participate in pattern matching. Implicitly scala compiler will add object for case class with a factory method apply. Pattern matching is a generalization of switch of Java.</p> <p>The syntax looks like:  <code>expr match { case pattern =&gt; expr ... case pn =&gt; en}</code></p> <ul> <li>in case of not matching there is a MatchError exception.</li> <li>If a pattern matches the whole right hand side is used.</li> <li>patterns are constructed from constructors, variables, wildcard patterns, constants</li> <li>case and pattern matching is also helpful for tree-like recursive data</li> <li>variable start by lowercase, where constant starts by Uppercase</li> </ul>"},{"location":"scala_summary/#flow-control","title":"Flow control","text":"<p>See the code example in: LeaningScala2.sc</p> <p>Almost all of Scala's control structures result in some value. This is the approach taken by functional languages, in which programs are viewed as computing a value, thus the components of a program should also compute values.</p> <pre><code>val filename =\n      if (!args.isEmpty) args(0)\n      else \"default.txt\"\n// while and do while: are loops not expression\n\n  var line = \"\"\n    do {\n      line = readLine()\n      println(\"Read: \"+ line)\n    } while (line != \"\")\n</code></pre> <p>The <code>while</code> and <code>do-while</code> constructs are called \"loops,\" not expressions, because they don't result in an interesting value. The type of the result is Unit.</p> <p><code>For</code> is simply used to iterate through a sequence of integers. But has more advanced expressions to iterate over multiple collections of different kinds, to filter out elements based on arbitrary conditions, and to produce new collections. The notation \" &lt;- \"  is called a generator,</p> <pre><code>for (s) yield e\n</code></pre> <p><code>s</code> is a sequence of generators and filters, <code>e</code> is an expression whose value is returned by an iteration generator of the form p &lt;- c   :  c is a collection and p is a pattern filter is a boolean expression if there are several generators in sequence the last ones vary faster than the first</p> <pre><code>// list the file in the current directory\nval filesHere = (new java.io.File(\".\")).listFiles\n\nfor (file &lt;- filesHere)\n  println(file)\n\nfor (i &lt;- 1 to 4)  // could be 1 until 4\n  println(\"Iteration \"+ i)\n\n// For supports filter to build subset\nfor (file &lt;- filesHere if file.getName.endsWith(\".scala\"))\n\n// filter can be combined, and separated by ;\nfor (\n      file &lt;- filesHere\n      if file.isFile;\n      if file.getName.endsWith(\".scala\")\n    )\n\n// If you add multiple &lt;- clauses, you will get nested \"loops.\"\n// for like a SQL query\nfor (b &lt;- books; a &lt;- b.authors if a startsWith \"Bird\") yield b.title\n</code></pre> <p><code>For</code> is closely related to higher order functions. For example the following three functions can be written with <code>for</code>:</p> <pre><code>def mapFunction[T,U] (xs : List[T], f: T =&gt; U) : List[U] =\n   for (x &lt;- xs) yield f(x)\n\ndef flatMap [T,U] (xs : List[T], f: T =&gt; Iterable[U]) : List[U] =\n   for (x &lt;- xs; y&lt;- f(x)) yield y\n\ndef filter2[T] (xs : List[T], f: T =&gt; Boolean) : List[T] =\n   for (x &lt;- xs; if f(x)) yield x\n\nval l=List( 2, 3, 4, 5)                    &gt; l  : List[Int] = List(2, 3, 4, 5)\n// to avoid missing parameter type, we need to add [Int]\nval r=filter2[Int](l,(x =&gt; x% 2 == 0))    &gt; r  : List[Int] = List(2, 4)\n</code></pre> <p>Expressing for with HOF</p> <pre><code>for (b &lt;- books; a &lt;- b.authors if a startsWith \"Bird\") yield b.title   is the same as\n  books flatMap (b =&gt; for (a &lt;- b.authors if a startsWith \"Bird\") yield b.title)   or\n  books flatMap (b =&gt; for (a &lt;- b.authors withFilter (a=&gt; a startsWith \"Bird\")) yield b.title)   or\n  books flatMap (b =&gt; b.authors withFilter (a=&gt; a startsWith \"Bird\") map (y =&gt; y.title)\n</code></pre>"},{"location":"scala_summary/#data-structure","title":"Data Structure","text":"<p>See the code example in: LeaningScala4.sc</p>"},{"location":"scala_summary/#list","title":"List","text":"<p>Lists are immutable, and recursive and linear: access to the first element is much faster than accessing to the middle of end of the list. Lists are constructed from empty list Nil and the cons operation <code>::</code></p> <pre><code>val empty = List()\nval fruit = List(\"apple\",\"banana\",\"pear\")\nval matrix = List(List(1,0,0),List(0,1,0),List(0,0,1))\n// equivalent to \nval fruit = \"apple\" :: (\"banana\" :: (\"pear\" :: Nil))\nval fruit = \"apple\" :: \"banana\" :: \"pear\" :: Nil\n// concat\nval fruits = fruit ++ List(\"strawberry\")\n</code></pre> <p>in scala there is a convention the operators ending with : associate to the right operand. They could be seen as method calls of the right hand operand Three list operations are important, head, tail, isEmpty</p> <pre><code>xs.length\nxs.last\nxs.init         // build a new list from xs without the last one\nxs take n    // return a new list of the n first elements of xs\nxs drop n    //the rest of the collection after position n\nxs(n)         // element at n position\nxs splitAt n   // return a pair of two sub lists from list xs one up to index n and one from position n\n\nval alist = \"aba\".toList\nalist groupBy(c =&gt; c)\n</code></pre> <p>Partitions a collection into a map of collections according to a discriminator function (here is Identity)</p> <pre><code>Map[Char,List[Char]] = Map(b -&gt; List(b), a -&gt; List(a, a))\nval m = Map(\"kirk\" -&gt; \"Enterprise\",\"joe\" -&gt; \"Voyager\")\nprintln(m[\"kirk\"])\n</code></pre> <p>It is possible to decompose a list with pattern matching:</p>"},{"location":"scala_summary/#nil-constant","title":"Nil constant","text":"<p>p :: ps is a pattern that matches a list with a head and a tail List(a,b,c,) to match a complete list</p> <p>Here are examples of list using recursion and pattern matching</p> <pre><code> def last[T] (xs : List[T]) : T = xs match {\n   case List() =&gt; throw new Error(\"Last of empty list is not possible\" )\n   case List(x) =&gt; x\n   case y :: ys =&gt; last(ys)\n   }                                              //&gt; last: [T](xs: List[T])T\n\n\ndef init[T] (xs : List[T]) : List[T] = xs match {\n   case List() =&gt; throw new Error(\"init of empty list is not possible\" )\n   case List(x) =&gt; List()\n   case y :: ys =&gt; y :: init(ys)\n  }\n\ndef concat[T](l1: List[T],l2 : List[T]) : List[T] =\n  l1 match {\n    case List() =&gt; l2\n    case y :: ys =&gt; y :: concat(ys,l2)\n  }\n</code></pre> <p>See the scala.collection.immutable.List documentation</p>"},{"location":"scala_summary/#other-collections","title":"Other collections","text":"<p>Vector has a more evenly balanced structure using 32 word array. when it is more than 32 elements a next level is added as array of pointer so covering 2** 10 elements.  The access is in log 32(n) it supports the same operations as on List as List and Vector are Seq, the class of all sequences which itself is a Iterable. Exception is on concatenation. </p> <pre><code> val nums= Vector( 1, 2, 3, 44)         &gt; nums  : scala.collection.immutable.Vector[Int] = Vector(1, 2, 3, 44)\n val n2 = 10 +: nums                  &gt; n2  : scala.collection.immutable.Vector[Int] = Vector(10, 1, 2, 3, 44)\n val n3 = nums :+ 15\n</code></pre>"},{"location":"scala_summary/#arrays-and-string-are-seq-too","title":"Arrays and String are Seq too","text":"<pre><code>  val a = Array( 1, 2, 3, 44)                &gt; a  : Array[Int] = Array(1, 2, 3, 44)\n  a map (x =&gt; x* 2)                         res0: Array[Int] = Array(2, 4, 6, 88)\n  val s = \"Hello World\"                   &gt; s  : String = Hello World\n  s filter (c =&gt; c.isUpper)/               &gt; res1: String = HW\n</code></pre> <p>range represents a sequence of evenly spaced integers. Some operations on Seq</p> <pre><code>val s = \"Hello World\"  \ns exists ( c =&gt; c == 'e') true if there is an element in s the p(x) holds\n\nxs forall p   true if p(x) holds for all elements x of xs\n\nxs zip ys     build a sequence of pairs drawn from corresponding elements of sequence xs and ys\nval z=s zip nums      //&gt; z  : scala.collection.immutable.IndexedSeq[(Char, Int)] = Vector((H,1), (e,2), (l,3), (l,44))\n  z.unzip               &gt; res3: (scala.collection.immutable.IndexedSeq[Char], scala.collection.immutable.IndexedSeq[Int]) = (Vector(H, e, l, l),Vector(1, 2, 3, 44))\n// other example using a range\nval q = List( 2, 0, 3)  //&gt; q  : List[Int] = List(2, 0, 3)\n(2 to 0 by -1) zip q /&gt; res0: scala.collection.immutable.IndexedSeq[(Int, Int)] = Vector((2,2), (1,0), (0,3))\n\nxs.unzip        Splits a sequence of pairs into two sequences consisting of the first and second halves of all pairs \nxs.flatMap f    applies collection-valued function f to all elements of x and concatenates the results\n\n// for example build a list of all combinations of number x for 1..M and y 1..N\n(1 to 5) flatMap ( x =&gt; ( 1 to 6) map (y =&gt; (x,y)))\n&gt; Vector((1,1), (1,2), (1,3), (1,4), (1,5), (1,6), (2,1), (2,2), (2,3), (2,4), (2,5), (2,6),\n\nExample of scalar product function: sum xi * yi\n\ndef scalarProduct(xs : Vector[Double], ys : Vector[Double]): Double =\n   (xs zip ys).map(xy =&gt; xy._1 * xy._2).sum     \n\nscalarProduct(Vector(2,3,4),Vector(3,5,8)) \n\n// compute if n is prime: if only divisors of n is 1 and n\ndef isPrime(n : Int) : Boolean =\n   (2 until n) forall (x =&gt; n % x != 0) \n</code></pre>"},{"location":"scala_summary/#getting-a-dictionary-of-word","title":"Getting a dictionary of word","text":"<p>The following will get each word (one per line in the input stream), transform it to a list and then filter out any word that does not contain a letter.  Filter apply a function to each member of the collection, forall apply for each element of the list a function f and return the matching elements.</p> <pre><code> val words = in.getLines.toList filter (word =&gt; word forall (c =&gt; c.isLetter))\n</code></pre>"},{"location":"scala_summary/#set","title":"Set","text":"<p>Is a sequence that does not have duplicate elements, elements are unordered, it includes the operation contains</p> <pre><code>  // return all the ways to encode a number to a set of strings\n  def encode(number: String) : Set[List[String]] =\n     if (number.isEmpty) Set(List())\n     else {\n       for {\n          split &lt;- 1 to number.length                      // from 1 to length\n          word &lt;- wordsForNum(number take split)           // cut the string number from 0 to split\n          rest &lt;- encode(number drop split)                // recursion on the rest of the number string\n       } yield word :: rest\n       }.toSet\n// returns  Set[List[String]]  ... List(Scala, is, fun), List(sack, bird, to)\n</code></pre> <p>Following code also illustrates that a function can be assigned to a variable. it is a map of digit strings to the words that represent them. words is list of words of a given dictionary</p> <pre><code>val wordsForNum : Map [String,Seq[String]] =\n    words groupBy wordCode withDefaultValue Seq()\n\n// wordsForNum  : Map[String,Seq[String]] = Map(63972278 -&gt; List(newscast),37638427 -&gt; List(cybernetics)\n</code></pre>"},{"location":"scala_summary/#map","title":"Map","text":"<p>maps are iterable and functional.</p> <pre><code> val roman = Map( \"I\" -&gt; 1 , \"II\" -&gt; 2 , \"V\" -&gt; 5 )   //&gt; roman  : scala.collection.immutable.Map[String,Int] = Map(I -&gt; 1, II -&gt; 2, V\n                                                  //|  -&gt; 5)\n roman(\"V\")                                       //&gt; res5: Int = 5\n roman get (\"II\")                                 //&gt; res6: Option[Int] = Some(2)\n</code></pre> <p>The Some is a class extending the trait Option type and takes a unique value. When a map does not contains a key it returns None which is declared as:</p> <pre><code>object None extends Option[Nothing]\n\nveggie groupBy (_.head)     &gt; res8: scala.collection.immutable.Map[Char,List[String]] = Map(b -&gt; List(brocoli), t -&gt; List(tom), c -&gt; List(carrot), o -&gt; List(olive))\n\n// mapValues Transforms a map by applying a function to every retrieved value\nveggie.groupBy(_.head).mapValues(_.length)\n</code></pre> <p>maps were partial functions this means that applying a map to a key value could lead to an exception if the key is not stored in the map. There is a function to set a default value if the key is not present</p> <pre><code>val c = veggie withDefaultValue \"Unknown\"\nc(\"x\")       // returns Unknown\n</code></pre>"},{"location":"scala_summary/#stream","title":"Stream","text":"<p>Similar to lists but their tail is evaluate only on demand. they are defined from a constant <code>Stream.empty</code> and a constructor <code>Stream.cons</code>. The following example illustrates that the interpreter does not build the full Stream</p> <pre><code> Stream.cons( 1,Stream.cons(2 ,Stream.Empty))      //&gt; res0: scala.collection.immutable.Stream.Cons[Int] = Stream(1, ?)\n Stream(1, 2,3 ,4)                                 //&gt; res1: scala.collection.immutable.Stream[Int] = Stream(1, ?)\n</code></pre> <p>Example of interesting use case. Compute the prime number from 1000 to 10000 but takes the second prime only. With a List the approach will compute all the prime while with Stream only the 2 first elements.</p> <pre><code>def isPrime(i: Int) : Boolean =\n    (2 until i) forall ( x =&gt; i % x != 0)         //&gt; isPrime: (i: Int)Boolean\n  ((1000 to 10000).toStream filter isPrime)( 1)   &gt; 1013\n\n// :: is not producing a Stream but..\nx #:: xs  == Stream.cons(x,xs)\n</code></pre>"},{"location":"scala_summary/#scala-build-test","title":"Scala Build Test","text":"<p>Using Scala Build Test. First start it in the folder with the project sub-folder, and eclipse .project</p> <pre><code>&gt; sbt\n&gt;styleCheck\n&gt;compile\n&gt;test\n&gt;run\n&gt;submit jerome.boyer@gmail.com  &lt;pwd&gt;  5Tz7bNHph7\n&gt;console\nscala &gt;\n</code></pre>"},{"location":"scala_summary/#scala-program","title":"Scala program","text":"<p>To run a Scala program, you must supply the name of a standalone singleton object with a main method that takes one parameter, an Array[String], and has a result type of Unit</p> <pre><code>object ThisIsAnApp {\n      def main(args: Array[String]) {\n        for (arg &lt;- args)\n          println(arg +\": \"+ calculate(arg))\n      }\n}\n</code></pre> <p>Scala provides a trait, scala.Application, to run an application without main function</p> <pre><code> object AnotherApp extends Application {\n\n      for (season &lt;- List(\"fall\", \"winter\", \"spring\"))\n        println(season +\": \"+ calculate(season))\n }\n ```\n\n The code between the curly braces is collected into a primary constructor of the singleton object, and is executed when the class is initialized\n\ncompile:\n\n```shell\nscalac  -d target/scala-2.10/classes\n</code></pre> <p>or use sbt &gt; compile</p> <pre><code># run unit test in sbt\n&gt; test\n</code></pre> <p>execute a program: <code>scala</code></p>"},{"location":"scala_summary/#integration-with-java","title":"Integration with java","text":"<p>scala can access java classes, extends java class and implement java interface. All classes from the java.lang package are imported by default, while others need to be imported explicitly.</p>"},{"location":"scala_summary/#further-readings","title":"Further Readings","text":"<ul> <li>Cheat sheets</li> </ul>"},{"location":"dev/","title":"Spark getting started","text":""},{"location":"dev/#basic-programming-concepts","title":"Basic programming concepts","text":""},{"location":"dev/#rdd-resilient-distributed-dataset","title":"RDD: Resilient Distributed Dataset","text":"<p>RDD is a dataset distributed against the cluster nodes. RDDs are fault-tolerant, immutable distributed collections of objects.  Each dataset in RDD is divided into logical partitions, which can be computed on different nodes of the cluster. To create a RDD, we use the spark context object and then one of its APIs depending of the data source (JDBC, Hive, HDFS, Cassandra, HBase, ElasticSearch, CSV, json,...). In the code below, <code>movies</code> variable is a RDD.</p> <pre><code>from pyspark import SparkConf, SparkContext\n\n# get spark session\nsparkConfiguration = SparkConf().setAppName(\"WorseMovie\")\nsparkSession = SparkContext(conf = sparkConfiguration)\n# load movie ratings from a csv file as a RDD\nmovies = sparkSession.textFile('../data/movielens/u.data')\nresults = movies.take(10)\nfor result in results:\n    print(result[0], result[1])\n</code></pre> <p>This program is not launched by using Python interpreter, but by the <code>spark-submit</code> tool. This tool is available in the Dockerfile we defined, with a python 3.7 interpreter.</p> <pre><code>/spark/bin/spark-submit nameoftheprogram.py\n</code></pre> <p>Creating a RDD can be done from different data sources, text file, csv, database, Hive, Cassandra...</p> <p>On Spark RDD, we can perform two kinds of operations: RDD Transformations and RDD actions.</p>"},{"location":"dev/#spark-context","title":"Spark context","text":"<p>Created by the driver program, it is responsible for making the RDD resilient and distributed. Here is an example of a special context creation for Spark Streaming, using local server with one executor per core, and using a batch size of 1 second.</p> <pre><code>val scc = new StreamingContext(\"local[*]\", \"TelemetryAlarmer\", Seconds(1))\n</code></pre>"},{"location":"dev/#transforming-rdds","title":"Transforming RDDs","text":"<p>Use <code>map, flatmap, filter, distinct, union, intersection, substract</code>, ... functions, and then applies one of the action.</p> <p>Nothing actually happens in your drive program until an action is called. Here is the python API documentation.</p> <p>The wordscale.scala code uses RDD to count word occurence in a text. To be able to get an executor running the code, the scala program needs to be an object and have a main function:</p> <pre><code>object wordcount {\n\n  def main(args: Array[String]) {\n  }\n}\n</code></pre> <ul> <li> <p><code>Map</code> transforms one row into another row:</p> <pre><code>// Now extract the text of each tweeter status update into DStreams:\nval statuses = tweets.map(status =&gt; status.getText())\n</code></pre> </li> <li> <p>while <code>mapFlat</code> transforms one row into multiple ones:</p> <pre><code>// Blow out each word into a new DStream\n    val tweetwords = statuses.flatMap(tweetText =&gt; tweetText.split(\" \"))\n</code></pre> </li> <li> <p><code>filter</code> helps to remove row not matching a condition:</p> <pre><code>    // Now eliminate anything that's not a hashtag\n    val hashtags = tweetwords.filter(word =&gt; word.startsWith(\"#\"))\n</code></pre> </li> </ul> <p>A classical transformation,  is to create key-value pair to count occurence of something like words using a reduce approach. <code>reduce(f,l)</code> applies the function f to elements of the list l by pair: (i,j) where i is the result of f(i-1,j-1).</p> <pre><code>valrdd.reduce((x,y) =&gt; x + y)\n</code></pre> <pre><code>// Map each hashtag to a key/value pair of (hashtag, 1) so we can count them up by adding up the values\nval hashtagKeyValues = hashtags.map(hashtag =&gt; (hashtag, 1))\n\nval counts = hashtagKeyValues.reduceByKey()\n</code></pre>"},{"location":"dev/#dataframes","title":"DataFrames","text":"<p>Spark 2.0 supports exposing data in RDD as data frames to apply SQL queries. DataFrame is a distributed collection of data organized into named columns. DataFrames contain Row Objects and may be easier to manipulate. </p> <p>In the example below, the movies rating file includes records like:</p> <pre><code>0   50  5   881250949\n0   172 5   881250949\n</code></pre> <p>The python code using RDD and data frame:</p> <pre><code>from pyspark.sql import SparkSession\nfrom pyspark.sql import Row\nfrom pyspark.sql import functions\n\ndef parseInput(line):\n    fields = line.split()\n    return Row(movieID = int(fields[1]), rating = float(fields[2]))\n\n# Create a SparkSession \nspark = SparkSession.builder.appName(\"PopularMovies\").getOrCreate()\nmovies = spark.sparkContext.textFile(\"../data/movielens/u.data\")\n# Convert it to a RDD of Row objects with (movieID, rating)\nmovieRows = lines.map(parseInput)\n# Convert that to a DataFrame\nmovieDataset = spark.createDataFrame(movieRows)\n</code></pre> <p>Then in data frame we can do SQL type of transformation</p> <pre><code> # Compute average rating for each movieID\naverageRatings = movieDataset.groupBy(\"movieID\").avg(\"rating\")\n\n# Compute count of ratings for each movieID\ncounts = movieDataset.groupBy(\"movieID\").count()\n</code></pre> <p>Since DataFrames are structure format that contains names and column, we can get the schema of the DataFrame using the <code>df.printSchema()</code>.</p>"},{"location":"dev/#scala","title":"Scala","text":""},{"location":"dev/#create-scala-project-with-maven","title":"Create scala project with maven","text":"<p>See this article to create a maven project for scala project, and package it. </p>"},{"location":"dev/#sbt-the-scala-cli","title":"SBT the scala CLI","text":"<p>Scala SBT is a tool to manage library dependencies for Scala development. It also helps to package all dependencies in a single jar.</p> <p>See sbt by examples note and this SBT essential tutorial.</p> <p>Example to create a project template: <code>sbt create scala/helloworld.g8</code>.</p> <p>Once code and unit tests done, package the scala program and then submit it to spark cluster:</p> <pre><code># In spark-studies/src/scala-wordcount\nsbt package\n# start a docker container with spark image (see previous environment notes)\ndocker run --rm -it --network spark_network -v $(pwd):/home jbcodeforce/spark bash\n# in the shell within the container\ncd /home\nspark-submit target/scala-2.12/wordcount_2.12-1.0.jar\n</code></pre>"},{"location":"dev/#proof-delta-lake-is-cool","title":"Proof Delta Lake is cool","text":"<ul> <li>Connect to the docker container:</li> </ul> <pre><code>docker exec -ti distracted_satoshi bash\n</code></pre> <ul> <li>Start spark-shell:</li> </ul> <pre><code>spark-shell\n</code></pre> <ul> <li>Write a code to create 100 records in a file with the classical dataframe API, and a second one that overwrites it but generates an exception:</li> </ul> <pre><code>// first job\nspark.range(100).repartition(1).write.mode(\"overwrite\").csv(\"./tmp/test/\")\n// second job\nscala.util.Try(spark.range(100).repartition(1).map{ i=&gt;\n    if (i&gt;50) {\n        Thread.sleep(5000)\n        throw new RuntimeException(\"Too bad crash !\")\n    }\n    i\n}.write.mode(\"overwrite\").csv(\"./tmp/test\"))\n</code></pre> <p>The <code>spark</code> is variable is a SparkSession and <code>sc</code> is the spark context, predefined in the shell.</p> <p>We should observe the file is deleted after the exception, so we lost data. As a general statement, due to the immutable nature of the underlying storage in the cloud, one of the challenges in data processing is updating or deleting a subset of identified records from a data lake.</p> <p>With Delta lake API we can keep the file created even if the second job could not complete the update. The code uses:</p> <pre><code>spark.range(100).select($\"id\".as(\"id\")).repartition(1).write.mode(\"overwrite\").format(\"delta\").save(\"./tmp/test/\")\n\nscala.util.Try(spark.range(100).repartition(1).map{ i=&gt;\n    if (i&gt;50) {\n        Thread.sleep(5000)\n        throw new RuntimeException(\"Too bad crash !\")\n    }\n    i\n}.select($\"value\".as(\"id\")).write.mode(\"overwrite\").format(\"delta\").save(\"./tmp/test\"))\n</code></pre> <p>It is important to note that now a transaction log was created under the <code>_delta_log</code> folder. And in the second job, the exception is created and delta could not create a commit file, so the first file is preserved. A read operation via the delta api will read file with a commit file only.</p> <p>(See source from Learning journal)</p> <p>Next step... Deployment with local run &gt;&gt;&gt;</p>"},{"location":"dev/deployment/","title":"Deployment options","text":"<p>info</p> <p>Updated 2/17/2023</p>"},{"location":"dev/deployment/#run-locally-with-docker","title":"Run locally with Docker","text":"<ul> <li> <p>Build the image: The Dockerfile, in this repository, is using a OpenJdk base image and install Spark 3.3.1. The command to build the images:</p> <pre><code>docker build -t jbcodeforce/spark .\n</code></pre> <p>(change the Spark version in the Dockerfile if there is a new release)</p> </li> <li> <p>Start the container with docker compose. The docker compose file is in the root directory of this repository. It uses the two scripts (start-master.sh and start-worker.sh) to start the master or worker automatically. The environment variables to parameterize the WEB UI port, master node URL, and master port are set in the docker compose file. Start the cluster with 3 workers.</p> </li> </ul> <pre><code>    docker-compose up --scale spark-worker=3\n\n    spark-master    | 19/12/31 22:48:22 INFO Master: Registering worker 172.19.0.4:39193 with 3 cores, 4.8 GB RAM\n    spark-master    | 19/12/31 22:48:22 INFO Master: Registering worker 172.19.0.3:39161 with 3 cores, 4.8 GB RAM\n    spark-master    | 19/12/31 22:48:22 INFO Master: Registering worker 172.19.0.5:43775 with 3 cores, 4.8 GB RAM\n    spark-worker_1  | 19/12/31 22:48:22 INFO Worker: Successfully registered with master spark://spark-master:7077\n    spark-worker_3  | 19/12/31 22:48:22 INFO Worker: Successfully registered with master spark://spark-master:7077\n    spark-worker_2  | 19/12/31 22:48:22 INFO Worker: Successfully registered with master spark://spark-master:7077\n</code></pre> <p>From there we should be able to run the different examples by starting another container on the same network:</p> <pre><code>docker run --rm -it --network spark-network -v $(pwd):/home jbcodeforce/spark bash\n</code></pre> Start with docker without docker compose <pre><code>docker run --rm -it --name spark-master --hostname spark-master -v $(pwd):/app \\\n        -p 7077:7077 -p 8085:8085 --network spark_network jbcodeforce/spark bash\n</code></pre> <p>Start the Spark Master node. Within the container run:</p> <p><pre><code>/spark/spark-3.3.2-bin-hadoop3/bin/spark-class org.apache.spark.deploy.master.Master --ip spark-master --port 7077 --webui-port 8085\n....\n19/12/31 21:24:56 INFO Utils: Successfully started service 'MasterUI' on port 8085.\n19/12/31 21:24:56 INFO MasterWebUI: Bound MasterWebUI to 0.0.0.0, and started at http://a63a4db062fb:8085\n19/12/31 21:24:56 INFO Master: I have been elected leader! New state: ALIVE\n</code></pre> Start a worker node, as a separate docker container</p> <pre><code>docker run --rm -it --name spark-worker --hostname spark-worker \\\n        --network spark_network jbcodeforce/spark bash\n</code></pre> <p>then within the bash shell:</p> <pre><code>/spark/spark-3.3.2-bin-hadoop3/bin/spark-class org.apache.spark.deploy.worker.Worker \\\n    --webui-port 8085 spark://spark-master:7077\n\n    ...\n    INFO Utils: Successfully started service 'sparkWorker' on port 45589.\n    19/12/31 21:39:58 INFO Worker: Starting Spark worker 172.18.0.3:45589 with 3 cores, 4.8 GB RAM\n    19/12/31 21:39:58 INFO Worker: Running Spark version 2.4.4\n    19/12/31 21:39:58 INFO Worker: Spark home: /spark\n    19/12/31 21:39:58 INFO Utils: Successfully started service 'WorkerUI' on port 8085.\n    19/12/31 21:39:58 INFO WorkerWebUI: Bound WorkerWebUI to 0.0.0.0, and started at http://spark-worker:8085\n    19/12/31 21:39:58 INFO Worker: Connecting to master spark-master:7077...\n    19/12/31 21:39:58 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.2:7077 after 75 ms (0 ms spent in bootstraps)\n    19/12/31 21:39:59 INFO Worker: Successfully registered with master spark://spark-master:7077\n</code></pre> <ul> <li> <p>Validate the installation, use the <code>spark-shell</code> inside the docker container as:</p> <pre><code>bash-4.4# cd /app\nbash-4.4# spark-shell\n\nUsing Scala version 2.12.15 (OpenJDK 64-Bit Server VM, Java 17.0.6)\nType in expressions to have them evaluated.\nType :help for more information.\n\nscala&gt; val rdd = sc.textFile(\"README.md\")\nrdd: org.apache.spark.rdd.RDD[String] = README.md MapPartitionsRDD[1] at textFile at &lt;console&gt;:24\nscala&gt; rdd.count()\n</code></pre> </li> <li> <p>Access the Spark console</p> <p>Simply go to: http://localhost:8085/</p> <p>On the Spark console we can see a new worker was added:</p> <p></p> <p>Note</p> <p>The docker image includes one script to start the master and one to start the workers. The docker compose file below uses those commands to propose a simple spark cluster with one worker and one master.  See below.</p> </li> </ul>"},{"location":"dev/deployment/#smoke-test-the-cluster","title":"Smoke test the cluster","text":"<p>Start a 3nd container instance to run the <code>spark-submit</code> command to compute the value of Pi:</p> <pre><code>docker run --rm -it --name spark-client --hostname spark-client \\\n            --network spark_network jbcodeforce/spark bash\n\n/spark/spark-3.3.2-bin-hadoop3/bin/spark-submit --master spark://spark-master:7077 --class     org.apache.spark.examples.SparkPi  /spark/spark-3.3.2-bin-hadoop3/examples/jars/spark-examples_2.12-3.3.2.jar 1000\n</code></pre>"},{"location":"dev/deployment/#spark-and-delta-lake","title":"Spark and Delta Lake","text":"<p>The dockerfile named <code>Dockerfile-deltalake</code> build a Spark with Delta Lake extension so we can use the API to save and load data with transaction support, schema validation.</p> <ul> <li>Build the image</li> </ul> <pre><code>docker build -f Dockerfile-deltalake -t jbcodeforce/spark-delta .\n</code></pre> <ul> <li>Run the image</li> </ul> <pre><code>docker run -p 4040:4040 -v $(pwd):/app jbcodeforce/spark-delta\n</code></pre> <ul> <li>If you need to update the version see the delta-spark compatibility list</li> </ul>"},{"location":"dev/deployment/#installation-on-k8s-or-openshift-cluster","title":"Installation on k8s or openshift cluster","text":"<p>The spark driver runs as pod. The driver creates executors, which are also running within Kubernetes pods, connects to them and then executes application code. The driver and executor pod scheduling is handled by Kubernetes.</p> <p>See this spark openshift deployment study.</p> <p>The spark product documentation or use a Spark operator. The basic steps are:</p> <ol> <li>Use a namespace like <code>jb-sparks</code>. Modify the cluster and operator yaml files to use this namespace.</li> <li> <p>Start the operator if it's not running</p> <pre><code>oc apply -f k8s-operators/operator.yaml\n</code></pre> </li> <li> <p>Add cluster: edit the file <code>k8s-operators/cluster.yaml</code> to configure your number of workers</p> <pre><code>oc apply -f k8s-operators/cluster.yaml\n</code></pre> </li> <li> <p>To verify the cluster runs do the following command on one of the worker pod to start a spark shell environment:</p> <pre><code> oc get pods\n # select one of the work pod\n oc exec -ti my-spark-cluster-w-5pcqw  bash\n bash-4.2$ spark-shell\n</code></pre> </li> </ol>"},{"location":"dev/examples/","title":"Some examples","text":""},{"location":"dev/examples/#assess-the-twitter-popular-hashtags","title":"Assess the Twitter popular Hashtags","text":"<p>The goal is to compute the most popular hashtag over a time window of 5 minutes and a slide interval of 1 seconds. See the solution in PopularHashtags.scala</p> <p>As seen previously, the approach is to get the tweet text, splits it by words, and then generates key value pair for \"(word, 1)\" tuples, then uses a specific reduce operation (<code>educeByKeyAndWindow</code>) using time window:</p> <pre><code>// Map each hashtag to a key/value pair of (hashtag, 1) so we can count them up by adding up the values\nval hashtagKeyValues = hashtags.map(hashtag =&gt; (hashtag, 1))\n\n// Now count them up over a 5 minute window sliding every one second\nval hashtagCounts = hashtagKeyValues.reduceByKeyAndWindow( (x,y) =&gt; x + y, (x,y) =&gt; x - y, Seconds(300), Seconds(1))\n</code></pre>"},{"location":"dev/python/","title":"Spark programming with Python","text":"<p>PySpark is the Python API to  perform real-time, large-scale data processing in a distributed environment using Python.</p> <p>See the product documentation to learn how to use pySpark.</p> <p>The advantages:</p> <ul> <li>Write Spark app in Python.</li> <li>Use interactive analysis of data in distributed environment.</li> <li>Pandas workload to any size by running it distributed across multiple nodes.</li> </ul>"},{"location":"dev/python/#major-constructs","title":"Major constructs","text":"<ul> <li>PySpark DataFrames are implemented on top of RDD.</li> <li> <p>PySpark applications start with initializing SparkSession</p> <pre><code>from pyspark.sql import SparkSession\n\nspark = SparkSession.builder.getOrCreate()\n</code></pre> </li> <li> <p>PySpark DataFrame can be created via <code>pyspark.sql.SparkSession.createDataFrame</code> typically by passing a list of lists, tuples, dictionaries and pyspark.sql.Rows, a pandas DataFrame and an RDD. It supports discovering the schema from the data, or explicit schema definition:</p> <pre><code>df = spark.createDataFrame([\n    (1, 2., 'string1', date(2000, 1, 1), datetime(2000, 1, 1, 12, 0)),\n    (2, 3., 'string2', date(2000, 2, 1), datetime(2000, 1, 2, 12, 0)),\n    (3, 4., 'string3', date(2000, 3, 1), datetime(2000, 1, 3, 12, 0))\n], schema='a long, b double, c string, d date, e timestamp')\n# show a summary of a data frame\ndf.select(\"a\", \"b\", \"c\").describe().show()\n</code></pre> </li> </ul>"},{"location":"dev/python/#coding-with-pyspark","title":"Coding with PySpark","text":""},{"location":"dev/python/#first-python-program","title":"First python program","text":"<p>See FirstSparkProgram.py code in src/samples folder.</p> <p>All Spark python program need a main function and then can use SparkContext or sql.SparkSession</p> <pre><code>if __name__ == \"__main__\":\n    sparkConfiguration = SparkConf().setAppName(\"App name\")\n    sparkSession = SparkContext(conf = sparkConfiguration)\n    # .. do a lot of things\n    sparkSession.stop()\n</code></pre> <p>The main function build a spark session, loads the data in a RDD and performs transformation or actions.</p> <pre><code>    sparkConfiguration = SparkConf().setAppName(\"Get POST requests\")\n    sparkSession = SparkContext(conf = sparkConfiguration)\n    # load text file in RDD\n    lines = sparkSession.textFile('../data/access_log.txt')\n    # get lines with POST trace\n    posts = lines.filter(lambda l: \"POST\" in l).collect()\n</code></pre> <ul> <li>To run it, be sure docker compose has started a master node and at least one worker node. </li> <li>Verify the Master console: http://localhost:8085/</li> <li> <p>Run the sample python program: To be able to run program as job on Spark cluster, we need to connect to the cluster and use <code>spark-submit</code> command. </p> <p>For that we are using another container instance, with the source code mounted to <code>/home</code>:</p> <pre><code>docker run --rm -it --network spark-network -v $(pwd):/home jbcodeforce/spark bash\n</code></pre> <p>In the shell within this container runs:</p> <pre><code>bash-5.2# cd /home/src\nbash-5.2# spark-submit samples/FirstSparkProgram.py\n</code></pre> </li> </ul> <p>The traces illustrate the start of the Executor, the creation of a SparkContext, the scheduling of a job,  the creation of a Python Runner, the DAG and the task executions.</p>"},{"location":"dev/python/#computing-the-lowest-rated-movie","title":"Computing the lowest rated movie","text":"<p>The approach is to read the rating file and map each line to a SQL Row(movieID , rating) then transform it in data frame. From the DataFram it is easy to compute average rating for each movieID, and counts the number of time the movie is rated, joins the two data frames and finally pulls the top 10 results:</p> <p>See the code in LowestRatedMovieDataFrame.py</p> <pre><code>bash-4.4# spark-submit samples/LowestRatedMovieDataFrame.py\n\nAmityville: Dollhouse (1996) 1.0\nSomebody to Love (1994) 1.0\nEvery Other Weekend (1990) 1.0\nHomage (1995) 1.0\n3 Ninjas: High Noon At Mega Mountain (1998) 1.0\nBird of Prey (1996) 1.0\nPower 98 (1995) 1.0\nBeyond Bedlam (1993) 1.0\nFalling in Love Again (1980) 1.0\nT-Men (1947) 1.0\n</code></pre>"},{"location":"dev/python/#assessing-similar-movies","title":"Assessing similar movies","text":"<p>This example is using Pandas with Spark to merge two files: movie rating and movie data. Spark context has the <code>read_text</code> function from different files into a single RDD. Then the code transforms this RDD in data frame, and uses pivot table.</p>"},{"location":"dev/python/#movie-recommendations","title":"Movie recommendations","text":"<p>It reads the rating file and maps each line to a SQL Row(userID , movieID , rating) then transforms it in data frame so it can apply ML recommendation using the Alternating Least Squares API on the dataframe. Once the model is fitted, takes the movies with at least 100 ratings, builds a test dataframe with the movie evaludated by user 0. From those movies, uses the model to do recommendations, finally gets the top 20 movies with the highest predicted rating for this user.</p> <p>See the code in MovieRecommendationsALS.py</p> <pre><code>bash-5.2# spark-submit samples/MovieRecommendationsALS.py\n\nClerks (1994) 5.125946044921875\nDie Hard (1988) 5.088437557220459\nStar Wars (1977) 5.009941101074219\nArmy of Darkness (1993) 4.961264610290527\nEmpire Strikes Back, The (1980) 4.9492716789245605\nAlien (1979) 4.911722183227539\nFrighteners, The (1996) 4.8579559326171875\nReservoir Dogs (1992) 4.808855056762695\nRaiders of the Lost Ark (1981) 4.786505222320557\nStar Trek: The Wrath of Khan (1982) 4.760307312011719\nTerminator, The (1984) 4.759642124176025\n...\n</code></pre>"},{"location":"dev/python/#deeper-dive","title":"Deeper dive","text":"<ul> <li>RDD pyspark API</li> <li>PySpark RDD, Dataframe and SQL examples from spark-examples github also explained in pyspark-tutorial.</li> </ul>"},{"location":"dev/streaming/","title":"Streaming","text":""},{"location":"dev/streaming/#spark-streaming","title":"Spark Streaming","text":"<p>Big data never stops, so there is a need to continuously analyze data streams in real time. Some nice use cases: clickstream, real time sensor data from IoT. </p> <p></p> <p>The RDD processing is distributed on different worker nodes to process data in parallel.</p> <p>It also use the DStreams, or Discretized Streams, which is a continuous stream of data that receives input from various sources like Kafka, Flume, Kinesis, or TCP sockets. DStreams is a collection of many RDDs, one for each time step, and may produce output at each time step too. It acts as a RDD at the global level but we can also access the underlying RDDs. We can apply stateless transformations on Dstreams, like map, filter, reduceByKey... or we can use stateful data to maintain long-lived state. This is used for aggregate.</p> <p>Stateless transformations are capable of combining data from many DStreams within each time step.</p> <p>While Stateful transformation uses data or intermediate results from previous batches and computes the result of the current batch. They track data across time.</p> <p>Spark streaming supports windowed transformation, to compute results across a longer time period than your batch interval. Can be used for example to compute the total product sell over a 1 hour time window. The windows slides as time goes one, to represent batches within the window interval.</p> <p>In fact there are three intervals:</p> <ul> <li>the batch interval is how often data is captured into a DStream. It is specified when defining the spark streaming context.</li> <li>the slide interval is how often a windowed transformation is computed</li> <li>the window interval is how far back in time the windowed transformation goes.</li> </ul> <p>To ensure fault tolerance incoming data is replicated to at least 3 worker nodes. For stateful operation, a checkpoint directory can be used to store states in case of failure and node restarts.</p> <p>The architecture of the receiver impacts the fault tolerance, for example if the receiver is a single point of failure. If a Twitter receiver fails, you loose data.</p> <p>The drive code can be also a SPOF. But there are ways to design and implement a more reliable driver, by using StreamingContext.getOrCreate() API and use checkpoint directory on distributed filesystem, to be used when the driver restarts, to pickup from the checkpoint directory.</p>"},{"location":"dev/streaming/#first-streaming-program","title":"First streaming program","text":"<p>Print tweets from twitter feed. You need a twitter account and API access, then populate the credential in a twitter.txt file.</p> <pre><code>consumerKey Zk6zkTS..\nconsumerSecret WIyK..\naccessToken 13...\naccessTokenSecret VhT..\n</code></pre> <p>Then in scala define a function to load such credentials as properties</p> <pre><code>def setupTwitter() = {\n    import scala.io.Source\n    for (line &lt;- Source.fromFile(\"../twitter.txt\").getLines) {\n      val fields = line.split(\" \")\n      if (fields.length == 2) {\n        System.setProperty(\"twitter4j.oauth.\" + fields(0), fields(1))\n      }\n    }\n  }\n</code></pre> <pre><code>import org.apache.spark.streaming.twitter._\n\nobject PrintTweets {\n  def main(args: Array[String]) {\n       val ssc = new StreamingContext(\"local[*]\", \"PrintTweets\", Seconds(1))\n       // Create a DStream from Twitter using our streaming context\n       val tweets = TwitterUtils.createStream(ssc, None)\n        // Now extract the text of each status update into RDD's using map()\n        val statuses = tweets.map(status =&gt; status.getText())\n        // Print out the first ten\n        statuses.print()\n        // Kick it all off\n        ssc.start()\n        ssc.awaitTermination()\n  }\n</code></pre>"}]}